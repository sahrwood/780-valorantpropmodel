{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bda38423-f7f1-4137-a674-4711fd7c65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_86552/4246683152.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n",
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_86552/4246683152.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Performance ===\n",
      "Train MAE: 2.589\n",
      "Test MAE: 3.280\n",
      "Train RMSE: 3.309\n",
      "Test RMSE: 4.051\n",
      "Train R²: 0.308\n",
      "Test R²: 0.114\n",
      "\n",
      "Feature Importances:\n",
      "pred_kills: 0.4460\n",
      "pred_kast_pct: 0.2314\n",
      "agent_coef: 0.3226\n",
      "\n",
      "Best Hyperparameters:\n",
      "gamma: 0.15\n",
      "learning_rate: 0.01\n",
      "max_depth: 3\n",
      "reg_alpha: 1\n",
      "reg_lambda: 1\n",
      "\n",
      "Holdout set size (rows): 1200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "#Player projections using decay\n",
    "\n",
    "# Parameters\n",
    "DECAY = 0.05\n",
    "PATCH_MULTIPLIER = 25\n",
    "\n",
    "# Distance between patches\n",
    "def custom_patch_distance(train_patch, holdout_patch):\n",
    "    hmaj, hmin = holdout_patch\n",
    "    tmaj, tmin = train_patch\n",
    "    return (hmaj - tmaj) * PATCH_MULTIPLIER + (hmin - tmin)\n",
    "\n",
    "df = pd.read_csv(\"/Users/samharwood/Downloads/statstester.csv\")\n",
    "df['patch_tuple'] = df['match_patch'].apply(lambda x: tuple(map(int, str(x).split('.'))))\n",
    "\n",
    "#Training and holdout split\n",
    "unique_patches = sorted(df['patch_tuple'].unique(), reverse=True)\n",
    "most_recent_patch = unique_patches[0]\n",
    "holdout_df = df[df['patch_tuple'] == most_recent_patch].copy()\n",
    "train_df = df[df['patch_tuple'] != most_recent_patch].copy()\n",
    "\n",
    "#Holdout split; 1000 matches from most recent patch taken for holdout. Default was to include all matches on the most recent patch in the holdout, which would prevent predictions to be informed by the most recent, relevant data\n",
    "if len(holdout_df) > 1000:\n",
    "    holdout_df = holdout_df.sample(n=1000, random_state=42)\n",
    "    remaining = df[(df['patch_tuple'] == most_recent_patch) & (~df.index.isin(holdout_df.index))]\n",
    "    train_df = pd.concat([train_df, remaining])\n",
    "\n",
    "# Filter players with >=5 matches in training\n",
    "valid_players = train_df.groupby('player').filter(lambda g: len(g) >= 5)['player'].unique()\n",
    "train_df = train_df[train_df['player'].isin(valid_players)].copy()\n",
    "\n",
    "# Decayed feature averaging\n",
    "def weighted_avg_feature(df, feature):\n",
    "    df['weight'] = np.exp(-DECAY * df['patch_tuple'].apply(\n",
    "        lambda x: custom_patch_distance(x, most_recent_patch)))\n",
    "    return df.groupby('player').apply(\n",
    "        lambda g: np.average(g[feature], weights=g['weight'])\n",
    "    ).reset_index(name=f'pred_{feature}')\n",
    "\n",
    "player_pred_kills = weighted_avg_feature(train_df.copy(), 'kills')\n",
    "player_pred_kast_pct = weighted_avg_feature(train_df.copy(), 'kast_pct')\n",
    "\n",
    "player_perf = player_pred_kills.merge(player_pred_kast_pct, on='player')\n",
    "\n",
    "#Agent level coefficients from prior code\n",
    "\n",
    "agent_coef_dict = {\n",
    "    'Reyna':4.0584, 'Iso':3.9195, 'Raze':3.3379, 'Neon':2.7851,\n",
    "    'Jett':2.6998, 'Yoru':2.3374, 'Chamber':2.1834, 'Sage':1.6417,\n",
    "    'Clove':1.4078, 'Viper':1.3776, 'Phoenix':0.8329, 'Brimstone':0.6529,\n",
    "    'Vyse':0.5889, 'Gekko':0.4869, 'Deadlock':0.2158, 'Tejo':0.1085,\n",
    "    'Killjoy':-0.0277, 'Fade':-0.0562, 'Sova':-0.0831, 'Omen':-0.2596,\n",
    "    'Astra':-0.6056, 'Skye':-0.8775, 'Kayo':-0.9327, 'Breach':-1.3188,\n",
    "    'Harbor':-3.4416, 'Cypher':0.0\n",
    "}\n",
    "\n",
    "agent_counts = train_df.groupby(['player', 'agent']).size().unstack(fill_value=0)\n",
    "all_agents = set(agent_counts.columns) | set(agent_coef_dict.keys())\n",
    "for agent in all_agents:\n",
    "    if agent not in agent_coef_dict:\n",
    "        agent_coef_dict[agent] = 0.0\n",
    "\n",
    "expected_coef = agent_counts.mul(agent_coef_dict).sum(axis=1).div(\n",
    "    agent_counts.sum(axis=1)).rename('agent_coef')\n",
    "\n",
    "player_perf = player_perf.merge(\n",
    "    expected_coef.reset_index(), \n",
    "    on='player', \n",
    "    how='left'\n",
    ").fillna(np.mean(list(agent_coef_dict.values())))\n",
    "\n",
    "#Combined dataset\n",
    "\n",
    "holdout_perf = holdout_df.groupby('player')['kills'].mean().reset_index(name='actual_kills')\n",
    "combined = player_perf.merge(holdout_perf, on='player', how='inner')\n",
    "\n",
    "X = combined[['pred_kills', 'pred_kast_pct', 'agent_coef']]\n",
    "y = combined['actual_kills']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#XGBoost with Grid Search\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.02, 0.03],\n",
    "    'reg_alpha': [0.5, 1],\n",
    "    'reg_lambda': [0.5, 1],\n",
    "    'gamma': [0, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='rmse',\n",
    "    verbosity=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    xgb, param_grid,\n",
    "    cv=TimeSeriesSplit(3),\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "\n",
    "train_pred = best_model.predict(X_train)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Model Performance ===\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train, train_pred):.3f}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, test_pred):.3f}\")\n",
    "print(f\"Train RMSE: {sqrt(mean_squared_error(y_train, train_pred)):.3f}\")\n",
    "print(f\"Test RMSE: {sqrt(mean_squared_error(y_test, test_pred)):.3f}\")\n",
    "print(f\"Train R²: {r2_score(y_train, train_pred):.3f}\")\n",
    "print(f\"Test R²: {r2_score(y_test, test_pred):.3f}\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, imp in zip(X.columns, best_model.feature_importances_):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in grid.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "\n",
    "print(f\"\\nHoldout set size (rows): {holdout_df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926e86ee-90fd-4552-91fd-6fdea6ba4c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             agent_coef  first_kills  kast_pct\n",
      "agent_coef     1.000000     0.809385 -0.481137\n",
      "first_kills    0.809385     1.000000 -0.661731\n",
      "kast_pct      -0.481137    -0.661731  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Comparison of agent stats to detemrine performance metrics\n",
    "agent_stats = df.groupby('agent').agg({\n",
    "    'first_kills': 'mean',\n",
    "    'kast_pct': 'mean',\n",
    "    'kills': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge with agent coeffs\n",
    "agent_stats = agent_stats.merge(\n",
    "    pd.DataFrame.from_dict(agent_coef_dict, orient='index', columns=['agent_coef']),\n",
    "    left_on='agent', right_index=True\n",
    ")\n",
    "\n",
    "# Compute correlations\n",
    "print(agent_stats[['agent_coef', 'first_kills', 'kast_pct']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9994c1c4-74b5-4218-ae59-4653977fd4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_73154/2378998353.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n",
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_73154/2378998353.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Two-Map Model Performance ===\n",
      "Train MAE: 4.173\n",
      "Test MAE: 6.507\n",
      "Train RMSE: 5.275\n",
      "Test RMSE: 8.006\n",
      "Train R²: 0.599\n",
      "Test R²: 0.258\n",
      "\n",
      "Feature Importances:\n",
      "pred_kills: 0.3916\n",
      "pred_kast_pct: 0.2768\n",
      "agent_coef: 0.3316\n",
      "\n",
      "Best Hyperparameters:\n",
      "gamma: 0.1\n",
      "learning_rate: 0.01\n",
      "max_depth: 5\n",
      "reg_alpha: 0.5\n",
      "reg_lambda: 0.5\n"
     ]
    }
   ],
   "source": [
    "#Final 2 map, XGB model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "# Parameters\n",
    "DECAY = 0.05\n",
    "PATCH_MULTIPLIER = 25\n",
    "\n",
    "# Distance between patches\n",
    "def custom_patch_distance(train_patch, holdout_patch):\n",
    "    hmaj, hmin = holdout_patch\n",
    "    tmaj, tmin = train_patch\n",
    "    return (hmaj - tmaj) * PATCH_MULTIPLIER + (hmin - tmin)\n",
    "\n",
    "df = pd.read_csv(\"/Users/samharwood/Downloads/statstester.csv\")\n",
    "df['patch_tuple'] = df['match_patch'].apply(lambda x: tuple(map(int, str(x).split('.'))))\n",
    "\n",
    "# Training and holdout split\n",
    "unique_patches = sorted(df['patch_tuple'].unique(), reverse=True)\n",
    "most_recent_patch = unique_patches[0]\n",
    "holdout_df = df[df['patch_tuple'] == most_recent_patch].copy()\n",
    "train_df = df[df['patch_tuple'] != most_recent_patch].copy()\n",
    "\n",
    "#Holdout split; 1000 matches from most recent patch taken for holdout. Default was to include all matches on the most recent patch in the holdout, which would prevent predictions to be informed by the most recent, relevant data\n",
    "if len(holdout_df) > 1000:\n",
    "    holdout_df = holdout_df.sample(n=1000, random_state=42)\n",
    "    remaining = df[(df['patch_tuple'] == most_recent_patch) & (~df.index.isin(holdout_df.index))]\n",
    "    train_df = pd.concat([train_df, remaining])\n",
    "\n",
    "# Filter players with >=5 matches in training\n",
    "valid_players = train_df.groupby('player').filter(lambda g: len(g) >= 5)['player'].unique()\n",
    "train_df = train_df[train_df['player'].isin(valid_players)].copy()\n",
    "\n",
    "# Decayed feature averaging\n",
    "def weighted_avg_feature(df, feature):\n",
    "    df['weight'] = np.exp(-DECAY * df['patch_tuple'].apply(\n",
    "        lambda x: custom_patch_distance(x, most_recent_patch)))\n",
    "    return df.groupby('player').apply(\n",
    "        lambda g: np.average(g[feature], weights=g['weight'])\n",
    "    ).reset_index(name=f'pred_{feature}')\n",
    "\n",
    "player_pred_kills = weighted_avg_feature(train_df.copy(), 'kills')\n",
    "player_pred_kast_pct = weighted_avg_feature(train_df.copy(), 'kast_pct')\n",
    "\n",
    "player_perf = player_pred_kills.merge(player_pred_kast_pct, on='player')\n",
    "\n",
    "# Agent level coefficients from prior code\n",
    "agent_coef_dict = {\n",
    "    'Reyna': 4.0584, 'Iso': 3.9195, 'Raze': 3.3379, 'Neon': 2.7851,\n",
    "    'Jett': 2.6998, 'Yoru': 2.3374, 'Chamber': 2.1834, 'Sage': 1.6417,\n",
    "    'Clove': 1.4078, 'Viper': 1.3776, 'Phoenix': 0.8329, 'Brimstone': 0.6529,\n",
    "    'Vyse': 0.5889, 'Gekko': 0.4869, 'Deadlock': 0.2158, 'Tejo': 0.1085,\n",
    "    'Killjoy': -0.0277, 'Fade': -0.0562, 'Sova': -0.0831, 'Omen': -0.2596,\n",
    "    'Astra': -0.6056, 'Skye': -0.8775, 'Kayo': -0.9327, 'Breach': -1.3188,\n",
    "    'Harbor': -3.4416, 'Cypher': 0.0\n",
    "}\n",
    "\n",
    "agent_counts = train_df.groupby(['player', 'agent']).size().unstack(fill_value=0)\n",
    "all_agents = set(agent_counts.columns) | set(agent_coef_dict.keys())\n",
    "\n",
    "expected_coef = agent_counts.mul(agent_coef_dict).sum(axis=1).div(\n",
    "    agent_counts.sum(axis=1)).rename('agent_coef')\n",
    "\n",
    "player_perf = player_perf.merge(\n",
    "    expected_coef.reset_index(), \n",
    "    on='player', \n",
    "    how='left'\n",
    ").fillna(np.mean(list(agent_coef_dict.values())))\n",
    "\n",
    "# Combine with holdout actuals: here, actual_kills is averaged per map\n",
    "holdout_perf = holdout_df.groupby('player')['kills'].mean().reset_index(name='actual_kills')\n",
    "combined = player_perf.merge(holdout_perf, on='player', how='inner')\n",
    "\n",
    "# Multiplying kills by 2 so model learns, scales to 2 map totals\n",
    "combined['actual_kills_2maps'] = combined['actual_kills'] * 2\n",
    "\n",
    "# Define features and target accordingly.\n",
    "X = combined[['pred_kills', 'pred_kast_pct', 'agent_coef']]\n",
    "y = combined['actual_kills_2maps']  # Revised target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# XGBoost with Grid Search\n",
    "param_grid = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.02, 0.03],\n",
    "    'reg_alpha': [0.5, 1],\n",
    "    'reg_lambda': [0.5, 1],\n",
    "    'gamma': [0, 0.1, 0.15]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=1000,\n",
    "    early_stopping_rounds=50,\n",
    "    eval_metric='rmse',\n",
    "    verbosity=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    xgb, param_grid,\n",
    "    cv=TimeSeriesSplit(3),\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=0)\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# Evaluation on two-map outcomes\n",
    "train_pred = best_model.predict(X_train)\n",
    "test_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Two-Map Model Performance ===\")\n",
    "print(f\"Train MAE: {mean_absolute_error(y_train, train_pred):.3f}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, test_pred):.3f}\")\n",
    "print(f\"Train RMSE: {sqrt(mean_squared_error(y_train, train_pred)):.3f}\")\n",
    "print(f\"Test RMSE: {sqrt(mean_squared_error(y_test, test_pred)):.3f}\")\n",
    "print(f\"Train R²: {r2_score(y_train, train_pred):.3f}\")\n",
    "print(f\"Test R²: {r2_score(y_test, test_pred):.3f}\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, imp in zip(X.columns, best_model.feature_importances_):\n",
    "    print(f\"{name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in grid.best_params_.items():\n",
    "    print(f\"{param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12985cf1-6550-442a-a2dc-0ff0ffd99023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_73154/3555430506.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n",
      "/var/folders/cp/v9x7jh4x7f9b1v1bp5tjcjlm0000gn/T/ipykernel_73154/3555430506.py:45: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('player').apply(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.001, dropout=0.25, batch_size=64\n",
      "New best val_mae: 6.6014\n",
      "\n",
      "Testing lr=0.001, dropout=0.25, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.001, dropout=0.3, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best val_mae: 6.3693\n",
      "\n",
      "Testing lr=0.001, dropout=0.3, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.0002, dropout=0.25, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.0002, dropout=0.25, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.0002, dropout=0.3, batch_size=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing lr=0.0002, dropout=0.3, batch_size=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparams\n",
      "Best Parameters: {'learning_rate': 0.001, 'dropout_rate': 0.3, 'batch_size': 64}\n",
      "Best Validation MAE: 6.3693\n",
      "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/stepWARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x28ff80900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Final Test Performance\n",
      "Test MAE: 6.699\n",
      "Test RMSE: 8.049\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "#Prior feature engineering before NN\n",
    "df = pd.read_csv(\"/Users/samharwood/Downloads/statstester.csv\")\n",
    "df['patch_tuple'] = df['match_patch'].apply(lambda x: tuple(map(int, str(x).split('.'))))\n",
    "\n",
    "# Parameters\n",
    "DECAY = 0.05\n",
    "PATCH_MULTIPLIER = 25\n",
    "\n",
    "def custom_patch_distance(train_patch, holdout_patch):\n",
    "    hmaj, hmin = holdout_patch\n",
    "    tmaj, tmin = train_patch\n",
    "    return (hmaj - tmaj) * PATCH_MULTIPLIER + (hmin - tmin)\n",
    "\n",
    "# Split data\n",
    "unique_patches = sorted(df['patch_tuple'].unique(), reverse=True)\n",
    "most_recent_patch = unique_patches[0]\n",
    "holdout_df = df[df['patch_tuple'] == most_recent_patch].copy()\n",
    "train_df = df[df['patch_tuple'] != most_recent_patch].copy()\n",
    "\n",
    "if len(holdout_df) > 1000:\n",
    "    holdout_df = holdout_df.sample(n=1000, random_state=42)\n",
    "    remaining = df[(df['patch_tuple'] == most_recent_patch) & (~df.index.isin(holdout_df.index))]\n",
    "    train_df = pd.concat([train_df, remaining])\n",
    "\n",
    "# Filter players\n",
    "valid_players = train_df.groupby('player').filter(lambda g: len(g) >= 5)['player'].unique()\n",
    "train_df = train_df[train_df['player'].isin(valid_players)].copy()\n",
    "\n",
    "# Weighted averages\n",
    "def weighted_avg_feature(df, feature):\n",
    "    df['weight'] = np.exp(-DECAY * df['patch_tuple'].apply(\n",
    "        lambda x: custom_patch_distance(x, most_recent_patch)))\n",
    "    return df.groupby('player').apply(\n",
    "        lambda g: np.average(g[feature], weights=g['weight'])\n",
    "    ).reset_index(name=f'pred_{feature}')\n",
    "\n",
    "player_pred_kills = weighted_avg_feature(train_df.copy(), 'kills')\n",
    "player_pred_kast_pct = weighted_avg_feature(train_df.copy(), 'kast_pct')\n",
    "player_perf = player_pred_kills.merge(player_pred_kast_pct, on='player')\n",
    "\n",
    "# Agent coefficients\n",
    "agent_coef_dict = {\n",
    "    'Reyna':4.0584, 'Iso':3.9195, 'Raze':3.3379, 'Neon':2.7851,\n",
    "    'Jett':2.6998, 'Yoru':2.3374, 'Chamber':2.1834, 'Sage':1.6417,\n",
    "    'Clove':1.4078, 'Viper':1.3776, 'Phoenix':0.8329, 'Brimstone':0.6529,\n",
    "    'Vyse':0.5889, 'Gekko':0.4869, 'Deadlock':0.2158, 'Tejo':0.1085,\n",
    "    'Killjoy':-0.0277, 'Fade':-0.0562, 'Sova':-0.0831, 'Omen':-0.2596,\n",
    "    'Astra':-0.6056, 'Skye':-0.8775, 'Kayo':-0.9327, 'Breach':-1.3188,\n",
    "    'Harbor':-3.4416, 'Cypher':0.0\n",
    "}\n",
    "\n",
    "agent_counts = train_df.groupby(['player', 'agent']).size().unstack(fill_value=0)\n",
    "expected_coef = agent_counts.mul(agent_coef_dict).sum(axis=1).div(\n",
    "    agent_counts.sum(axis=1)).rename('agent_coef')\n",
    "player_perf = player_perf.merge(expected_coef.reset_index(), on='player', how='left').fillna(0)\n",
    "\n",
    "# Prepare final dataset\n",
    "holdout_perf = holdout_df.groupby('player')['kills'].mean().reset_index(name='actual_kills')\n",
    "combined = player_perf.merge(holdout_perf, on='player', how='inner')\n",
    "combined['actual_kills_2maps'] = combined['actual_kills'] * 2\n",
    "X = combined[['pred_kills', 'pred_kast_pct', 'agent_coef']]\n",
    "y = combined['actual_kills_2maps']\n",
    "\n",
    "#Preparing NN data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "#Grid Search\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.3, batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='swish', input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='swish'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='swish'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mae',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "#Grid params for testing\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.0002],\n",
    "    'dropout_rate': [0.25, 0.3],\n",
    "    'batch_size': [64, 128]\n",
    "}\n",
    "\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for dr in param_grid['dropout_rate']:\n",
    "        for bs in param_grid['batch_size']:\n",
    "            print(f\"\\nTesting lr={lr}, dropout={dr}, batch_size={bs}\")\n",
    "            \n",
    "            model = create_model(\n",
    "                learning_rate=lr,\n",
    "                dropout_rate=dr,\n",
    "                batch_size=bs\n",
    "            )\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=200,\n",
    "                batch_size=bs,\n",
    "                callbacks=[\n",
    "                    EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "                ],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            val_mae = min(history.history['val_mae'])\n",
    "            if val_mae < best_score:\n",
    "                best_score = val_mae\n",
    "                best_params = {\n",
    "                    'learning_rate': lr,\n",
    "                    'dropout_rate': dr,\n",
    "                    'batch_size': bs\n",
    "                }\n",
    "                best_model = model\n",
    "                print(f\"New best val_mae: {val_mae:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Best Hyperparams\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation MAE: {best_score:.4f}\")\n",
    "\n",
    "#Test Mae/RMSE\n",
    "y_pred = best_model.predict(X_test).flatten()\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"Final Test Performance\")\n",
    "print(f\"Test MAE: {test_mae:.3f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a6dbc-7269-4f4f-9bee-4b6eb9b4e0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
